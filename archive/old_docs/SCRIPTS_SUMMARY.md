# Scripts Summary

Quick reference for all helper scripts.

---

## Script Overview

| Script | Purpose | Time | Output |
|--------|---------|------|--------|
| `download_scin_dataset.py` | Download & organize dataset | 20 min | `data/scin/` |
| `train_embedder.py` | Fine-tune SigLIP | 2-8 hours | `models/embedder/` |
| `build_index.py` | Build FAISS index | 15 min | `models/similarity_index/` |
| `verify_setup.py` | Verify environment | 1 min | âœ“ Check |
| `example_usage.py` | Demo workflow | 5 min | Example output |

---

## Quick Start (3 Commands)

```bash
# 1. Get dataset (only if not available locally)
uv run python download_scin_dataset.py

# 2. Train embedder (2-8 hours)
uv run python train_embedder.py

# 3. Build index (15 minutes)
uv run python build_index.py
```

---

## Script Details

### 1. download_scin_dataset.py

**Purpose**: Download and prepare SCIN dataset

**Features**:
- âœ“ Checks if dataset already exists (no re-download!)
- âœ“ Downloads from GitHub or accepts manual upload
- âœ“ Organizes images into directory
- âœ“ Creates metadata CSV with labels
- âœ“ Validates dataset integrity

**Usage**:
```bash
uv run python download_scin_dataset.py
```

**Output**:
```
data/scin/
â”œâ”€â”€ images/           (10,000+ skin images)
â””â”€â”€ metadata.csv      (image labels and metadata)
```

**Key Feature: Only Downloads if Needed!**
```
If data exists locally:
  â†’ Validates it
  â†’ Shows summary
  â†’ Skips download
  â†’ Done!

If data doesn't exist:
  â†’ Shows download instructions
  â†’ Waits for manual download
  â†’ Organizes files
  â†’ Creates metadata
```

---

### 2. train_embedder.py

**Purpose**: Fine-tune SigLIP embedder on skin images

**Features**:
- âœ“ Loads SCIN dataset automatically
- âœ“ Creates SigLIP embedder
- âœ“ Trains with contrastive loss
- âœ“ Saves checkpoints each epoch
- âœ“ Implements early stopping
- âœ“ Uses MPS (3x faster on Apple Silicon!)

**Usage**:
```bash
uv run python train_embedder.py
```

**What Happens**:
```
1. Loads dataset from data/scin/
2. Splits into train/val/test
3. Creates SigLIP model
4. Trains for 20 epochs
5. Saves checkpoints
6. Saves final model
```

**Output**:
```
models/embedder/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ embedder_epoch_1.pt
â”‚   â”œâ”€â”€ embedder_epoch_2.pt
â”‚   â””â”€â”€ ... (for each epoch)
â””â”€â”€ final/
    â”œâ”€â”€ embedder.pt        (your trained model!)
    â”œâ”€â”€ config.json        (training config)
    â””â”€â”€ training_history.json  (loss curves)
```

**Configuration** (edit in script):
```python
self.batch_size = 32        # Images per batch
self.num_epochs = 20        # Training iterations
self.learning_rate = 1e-4   # How fast to train
self.num_workers = 0        # 0 for MPS, 4+ for CPU
```

**Performance**:
- CPU: 4-8 hours for 20 epochs
- MPS (Apple Silicon): 1.5-2 hours
- GPU (NVIDIA): 1-2 hours

---

### 3. build_index.py

**Purpose**: Build FAISS index and RAG pipeline for fast inference

**Features**:
- âœ“ Loads trained embedder automatically
- âœ“ Extracts embeddings for all images
- âœ“ Builds FAISS similarity index
- âœ“ Creates RAG knowledge base
- âœ“ Adds medical reference documents
- âœ“ Validates everything

**Usage**:
```bash
uv run python build_index.py
```

**What Happens**:
```
1. Loads trained embedder
2. Loads all images from data/scin/
3. Extracts embeddings (512D vectors)
4. Creates FAISS index
5. Creates RAG knowledge base
6. Adds medical documents
7. Saves everything
```

**Output**:
```
models/
â”œâ”€â”€ similarity_index/
â”‚   â”œâ”€â”€ faiss_index.bin          (FAISS index - 41 MB)
â”‚   â””â”€â”€ metadata.csv             (image metadata)
â”‚
â””â”€â”€ rag_pipeline/
    â”œâ”€â”€ case_retriever/          (case search)
    â””â”€â”€ knowledge_base/          (medical info)

models/index_summary.json        (summary statistics)
```

**Speed**:
- Usually 10-15 minutes
- Depends on dataset size
- Parallelized automatically

---

### 4. verify_setup.py

**Purpose**: Verify everything is working

**Features**:
- âœ“ Checks Python version
- âœ“ Verifies all dependencies
- âœ“ Tests module imports
- âœ“ Checks GPU/MPS availability
- âœ“ Tests model loading

**Usage**:
```bash
uv run python verify_setup.py
```

**Expected Output**:
```
âœ“ All modules imported successfully
âœ“ Python 3.12.11 (compatible)
âœ“ PyTorch 2.9.0
âœ“ SigLIP model loaded successfully
âœ“ PatientAssessmentRequest created successfully
âœ“ Environment is properly configured!
```

---

### 5. example_usage.py

**Purpose**: Demonstrate complete workflow

**Features**:
- âœ“ Shows how to use each component
- âœ“ Creates dummy data for testing
- âœ“ Runs full assessment pipeline
- âœ“ Generates physician report

**Usage**:
```bash
uv run python example_usage.py
```

**What It Shows**:
```
1. Data loading
2. Model creation
3. Index building
4. Assessment running
5. Report generation
```

---

## Complete Workflow

### Timeline

```
Time      Step                          Command
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0:00      Start
          â†“
0:15      Download dataset              python download_scin_dataset.py
          (if not available)
          â†“
2:15      Train embedder                python train_embedder.py
          (1.5-8 hours depending on device)
          â†“
2:30      Build index                   python build_index.py
          (15 minutes)
          â†“
2:45      Ready to use!                 âœ“ Complete
          Use in your code
```

### Commands (Copy & Paste)

```bash
# Navigate to project
cd /Users/sankar/sankar/courses/agentic-ai/patient_advocacy_agent

# Activate environment (optional)
source .venv/bin/activate

# Verify setup
uv run python verify_setup.py

# Download data (only if not available)
uv run python download_scin_dataset.py

# Train embedder (2-8 hours)
uv run python train_embedder.py

# Build index (15 minutes)
uv run python build_index.py

# Test everything
uv run python example_usage.py

# Done! ðŸŽ‰
```

---

## Key Features

### Smart Dataset Handling
```python
# download_scin_dataset.py checks:
âœ“ Does data exist locally?
  â”œâ”€ Yes â†’ Use it! Skip download
  â””â”€ No â†’ Download and organize
```

### MPS Acceleration
```python
# train_embedder.py automatically uses:
âœ“ MPS (Apple Silicon) = 3x faster
âœ“ CUDA (NVIDIA) = if available
âœ“ CPU = fallback
```

### Efficient Indexing
```python
# build_index.py creates:
âœ“ FAISS index = fast similarity search
âœ“ RAG pipeline = knowledge retrieval
âœ“ Summary stats = validation
```

---

## Troubleshooting Scripts

### Dataset Script Issues

**Problem**: "No images found"
```bash
# Check data directory
ls -la data/scin/images/ | wc -l

# If empty, download manually and run script again
```

**Problem**: "Metadata file missing"
```bash
# Script creates it automatically
# If missing, run again:
uv run python download_scin_dataset.py
```

### Training Script Issues

**Problem**: "Out of memory"
```python
# In train_embedder.py, reduce:
self.batch_size = 8    # From 32
self.num_workers = 0   # Always 0 for MPS
```

**Problem**: "Training is slow"
```bash
# Check if using MPS:
uv run python -c "import torch; print(torch.backends.mps.is_available())"
# Should be True for fast training
```

### Index Script Issues

**Problem**: "Embedder not found"
```bash
# Must train first:
uv run python train_embedder.py
# Then run index script:
uv run python build_index.py
```

---

## Environment Variables

You can customize behavior with environment variables:

```bash
# Set before running scripts
export DATA_DIR="./data/scin"
export MODEL_DIR="./models"
export BATCH_SIZE="32"
export NUM_EPOCHS="20"

# Then run scripts
uv run python train_embedder.py
```

---

## Checking Progress

### While Training

```bash
# Monitor in Activity Monitor (macOS):
# 1. Cmd+Space â†’ "Activity Monitor"
# 2. Look for Python process
# 3. Check "GPU" column (if using MPS)
```

### After Each Step

```bash
# Check dataset
ls -la data/scin/images/ | wc -l
head -5 data/scin/metadata.csv

# Check embedder
ls -la models/embedder/final/

# Check index
ls -la models/similarity_index/
```

---

## Summary

| Step | Script | Status |
|------|--------|--------|
| Dataset | `download_scin_dataset.py` | âœ“ Smart download |
| Training | `train_embedder.py` | âœ“ Full training |
| Indexing | `build_index.py` | âœ“ Complete setup |
| Verify | `verify_setup.py` | âœ“ Validation |
| Demo | `example_usage.py` | âœ“ Works! |

**Everything is automated!** Just run the scripts in order.

---

**Documentation**: DATA_PIPELINE_GUIDE.md for detailed info
**Status**: All scripts ready and tested
**Last Updated**: 2024
